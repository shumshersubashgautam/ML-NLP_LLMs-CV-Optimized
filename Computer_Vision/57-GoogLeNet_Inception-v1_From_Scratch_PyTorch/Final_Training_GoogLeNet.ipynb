{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQYTKcCS9LYB"
      },
      "source": [
        "## We are going to implement GoogLeNet and train it on CIFAR-10.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lh8f1QBFOP5O",
        "outputId": "eb25b531-933e-4f9a-91f0-44cb79e24e2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "thwqjGZx6CbQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision import transforms, datasets\n",
        "from torchsummary import summary\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "from GoogLeNet import *\n",
        "from train import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_Iwek5h6Cdx",
        "outputId": "cbeec1ea-f112-42db-fdac-26e660a93a8f",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 48, 48]           9,472\n",
            "       BatchNorm2d-2           [-1, 64, 48, 48]             128\n",
            "              ReLU-3           [-1, 64, 48, 48]               0\n",
            "         ConvBlock-4           [-1, 64, 48, 48]               0\n",
            "         MaxPool2d-5           [-1, 64, 24, 24]               0\n",
            "            Conv2d-6           [-1, 64, 24, 24]           4,160\n",
            "       BatchNorm2d-7           [-1, 64, 24, 24]             128\n",
            "              ReLU-8           [-1, 64, 24, 24]               0\n",
            "         ConvBlock-9           [-1, 64, 24, 24]               0\n",
            "           Conv2d-10          [-1, 192, 24, 24]         110,784\n",
            "      BatchNorm2d-11          [-1, 192, 24, 24]             384\n",
            "             ReLU-12          [-1, 192, 24, 24]               0\n",
            "        ConvBlock-13          [-1, 192, 24, 24]               0\n",
            "        MaxPool2d-14          [-1, 192, 12, 12]               0\n",
            "           Conv2d-15           [-1, 64, 12, 12]          12,352\n",
            "      BatchNorm2d-16           [-1, 64, 12, 12]             128\n",
            "             ReLU-17           [-1, 64, 12, 12]               0\n",
            "        ConvBlock-18           [-1, 64, 12, 12]               0\n",
            "           Conv2d-19           [-1, 96, 12, 12]          18,528\n",
            "      BatchNorm2d-20           [-1, 96, 12, 12]             192\n",
            "             ReLU-21           [-1, 96, 12, 12]               0\n",
            "        ConvBlock-22           [-1, 96, 12, 12]               0\n",
            "           Conv2d-23          [-1, 128, 12, 12]         110,720\n",
            "      BatchNorm2d-24          [-1, 128, 12, 12]             256\n",
            "             ReLU-25          [-1, 128, 12, 12]               0\n",
            "        ConvBlock-26          [-1, 128, 12, 12]               0\n",
            "           Conv2d-27           [-1, 16, 12, 12]           3,088\n",
            "      BatchNorm2d-28           [-1, 16, 12, 12]              32\n",
            "             ReLU-29           [-1, 16, 12, 12]               0\n",
            "        ConvBlock-30           [-1, 16, 12, 12]               0\n",
            "           Conv2d-31           [-1, 32, 12, 12]          12,832\n",
            "      BatchNorm2d-32           [-1, 32, 12, 12]              64\n",
            "             ReLU-33           [-1, 32, 12, 12]               0\n",
            "        ConvBlock-34           [-1, 32, 12, 12]               0\n",
            "        MaxPool2d-35          [-1, 192, 12, 12]               0\n",
            "           Conv2d-36           [-1, 32, 12, 12]           6,176\n",
            "      BatchNorm2d-37           [-1, 32, 12, 12]              64\n",
            "             ReLU-38           [-1, 32, 12, 12]               0\n",
            "        ConvBlock-39           [-1, 32, 12, 12]               0\n",
            "        Inception-40          [-1, 256, 12, 12]               0\n",
            "           Conv2d-41          [-1, 128, 12, 12]          32,896\n",
            "      BatchNorm2d-42          [-1, 128, 12, 12]             256\n",
            "             ReLU-43          [-1, 128, 12, 12]               0\n",
            "        ConvBlock-44          [-1, 128, 12, 12]               0\n",
            "           Conv2d-45          [-1, 128, 12, 12]          32,896\n",
            "      BatchNorm2d-46          [-1, 128, 12, 12]             256\n",
            "             ReLU-47          [-1, 128, 12, 12]               0\n",
            "        ConvBlock-48          [-1, 128, 12, 12]               0\n",
            "           Conv2d-49          [-1, 192, 12, 12]         221,376\n",
            "      BatchNorm2d-50          [-1, 192, 12, 12]             384\n",
            "             ReLU-51          [-1, 192, 12, 12]               0\n",
            "        ConvBlock-52          [-1, 192, 12, 12]               0\n",
            "           Conv2d-53           [-1, 32, 12, 12]           8,224\n",
            "      BatchNorm2d-54           [-1, 32, 12, 12]              64\n",
            "             ReLU-55           [-1, 32, 12, 12]               0\n",
            "        ConvBlock-56           [-1, 32, 12, 12]               0\n",
            "           Conv2d-57           [-1, 96, 12, 12]          76,896\n",
            "      BatchNorm2d-58           [-1, 96, 12, 12]             192\n",
            "             ReLU-59           [-1, 96, 12, 12]               0\n",
            "        ConvBlock-60           [-1, 96, 12, 12]               0\n",
            "        MaxPool2d-61          [-1, 256, 12, 12]               0\n",
            "           Conv2d-62           [-1, 64, 12, 12]          16,448\n",
            "      BatchNorm2d-63           [-1, 64, 12, 12]             128\n",
            "             ReLU-64           [-1, 64, 12, 12]               0\n",
            "        ConvBlock-65           [-1, 64, 12, 12]               0\n",
            "        Inception-66          [-1, 480, 12, 12]               0\n",
            "        MaxPool2d-67            [-1, 480, 6, 6]               0\n",
            "           Conv2d-68            [-1, 192, 6, 6]          92,352\n",
            "      BatchNorm2d-69            [-1, 192, 6, 6]             384\n",
            "             ReLU-70            [-1, 192, 6, 6]               0\n",
            "        ConvBlock-71            [-1, 192, 6, 6]               0\n",
            "           Conv2d-72             [-1, 96, 6, 6]          46,176\n",
            "      BatchNorm2d-73             [-1, 96, 6, 6]             192\n",
            "             ReLU-74             [-1, 96, 6, 6]               0\n",
            "        ConvBlock-75             [-1, 96, 6, 6]               0\n",
            "           Conv2d-76            [-1, 208, 6, 6]         179,920\n",
            "      BatchNorm2d-77            [-1, 208, 6, 6]             416\n",
            "             ReLU-78            [-1, 208, 6, 6]               0\n",
            "        ConvBlock-79            [-1, 208, 6, 6]               0\n",
            "           Conv2d-80             [-1, 16, 6, 6]           7,696\n",
            "      BatchNorm2d-81             [-1, 16, 6, 6]              32\n",
            "             ReLU-82             [-1, 16, 6, 6]               0\n",
            "        ConvBlock-83             [-1, 16, 6, 6]               0\n",
            "           Conv2d-84             [-1, 48, 6, 6]          19,248\n",
            "      BatchNorm2d-85             [-1, 48, 6, 6]              96\n",
            "             ReLU-86             [-1, 48, 6, 6]               0\n",
            "        ConvBlock-87             [-1, 48, 6, 6]               0\n",
            "        MaxPool2d-88            [-1, 480, 6, 6]               0\n",
            "           Conv2d-89             [-1, 64, 6, 6]          30,784\n",
            "      BatchNorm2d-90             [-1, 64, 6, 6]             128\n",
            "             ReLU-91             [-1, 64, 6, 6]               0\n",
            "        ConvBlock-92             [-1, 64, 6, 6]               0\n",
            "        Inception-93            [-1, 512, 6, 6]               0\n",
            "AdaptiveAvgPool2d-94            [-1, 512, 4, 4]               0\n",
            "           Conv2d-95            [-1, 128, 4, 4]          65,664\n",
            "             ReLU-96            [-1, 128, 4, 4]               0\n",
            "           Linear-97                 [-1, 1024]       2,098,176\n",
            "             ReLU-98                 [-1, 1024]               0\n",
            "          Dropout-99                 [-1, 1024]               0\n",
            "          Linear-100                   [-1, 10]          10,250\n",
            "       Auxiliary-101                   [-1, 10]               0\n",
            "          Conv2d-102            [-1, 160, 6, 6]          82,080\n",
            "     BatchNorm2d-103            [-1, 160, 6, 6]             320\n",
            "            ReLU-104            [-1, 160, 6, 6]               0\n",
            "       ConvBlock-105            [-1, 160, 6, 6]               0\n",
            "          Conv2d-106            [-1, 112, 6, 6]          57,456\n",
            "     BatchNorm2d-107            [-1, 112, 6, 6]             224\n",
            "            ReLU-108            [-1, 112, 6, 6]               0\n",
            "       ConvBlock-109            [-1, 112, 6, 6]               0\n",
            "          Conv2d-110            [-1, 224, 6, 6]         226,016\n",
            "     BatchNorm2d-111            [-1, 224, 6, 6]             448\n",
            "            ReLU-112            [-1, 224, 6, 6]               0\n",
            "       ConvBlock-113            [-1, 224, 6, 6]               0\n",
            "          Conv2d-114             [-1, 24, 6, 6]          12,312\n",
            "     BatchNorm2d-115             [-1, 24, 6, 6]              48\n",
            "            ReLU-116             [-1, 24, 6, 6]               0\n",
            "       ConvBlock-117             [-1, 24, 6, 6]               0\n",
            "          Conv2d-118             [-1, 64, 6, 6]          38,464\n",
            "     BatchNorm2d-119             [-1, 64, 6, 6]             128\n",
            "            ReLU-120             [-1, 64, 6, 6]               0\n",
            "       ConvBlock-121             [-1, 64, 6, 6]               0\n",
            "       MaxPool2d-122            [-1, 512, 6, 6]               0\n",
            "          Conv2d-123             [-1, 64, 6, 6]          32,832\n",
            "     BatchNorm2d-124             [-1, 64, 6, 6]             128\n",
            "            ReLU-125             [-1, 64, 6, 6]               0\n",
            "       ConvBlock-126             [-1, 64, 6, 6]               0\n",
            "       Inception-127            [-1, 512, 6, 6]               0\n",
            "          Conv2d-128            [-1, 128, 6, 6]          65,664\n",
            "     BatchNorm2d-129            [-1, 128, 6, 6]             256\n",
            "            ReLU-130            [-1, 128, 6, 6]               0\n",
            "       ConvBlock-131            [-1, 128, 6, 6]               0\n",
            "          Conv2d-132            [-1, 128, 6, 6]          65,664\n",
            "     BatchNorm2d-133            [-1, 128, 6, 6]             256\n",
            "            ReLU-134            [-1, 128, 6, 6]               0\n",
            "       ConvBlock-135            [-1, 128, 6, 6]               0\n",
            "          Conv2d-136            [-1, 256, 6, 6]         295,168\n",
            "     BatchNorm2d-137            [-1, 256, 6, 6]             512\n",
            "            ReLU-138            [-1, 256, 6, 6]               0\n",
            "       ConvBlock-139            [-1, 256, 6, 6]               0\n",
            "          Conv2d-140             [-1, 24, 6, 6]          12,312\n",
            "     BatchNorm2d-141             [-1, 24, 6, 6]              48\n",
            "            ReLU-142             [-1, 24, 6, 6]               0\n",
            "       ConvBlock-143             [-1, 24, 6, 6]               0\n",
            "          Conv2d-144             [-1, 64, 6, 6]          38,464\n",
            "     BatchNorm2d-145             [-1, 64, 6, 6]             128\n",
            "            ReLU-146             [-1, 64, 6, 6]               0\n",
            "       ConvBlock-147             [-1, 64, 6, 6]               0\n",
            "       MaxPool2d-148            [-1, 512, 6, 6]               0\n",
            "          Conv2d-149             [-1, 64, 6, 6]          32,832\n",
            "     BatchNorm2d-150             [-1, 64, 6, 6]             128\n",
            "            ReLU-151             [-1, 64, 6, 6]               0\n",
            "       ConvBlock-152             [-1, 64, 6, 6]               0\n",
            "       Inception-153            [-1, 512, 6, 6]               0\n",
            "          Conv2d-154            [-1, 112, 6, 6]          57,456\n",
            "     BatchNorm2d-155            [-1, 112, 6, 6]             224\n",
            "            ReLU-156            [-1, 112, 6, 6]               0\n",
            "       ConvBlock-157            [-1, 112, 6, 6]               0\n",
            "          Conv2d-158            [-1, 144, 6, 6]          73,872\n",
            "     BatchNorm2d-159            [-1, 144, 6, 6]             288\n",
            "            ReLU-160            [-1, 144, 6, 6]               0\n",
            "       ConvBlock-161            [-1, 144, 6, 6]               0\n",
            "          Conv2d-162            [-1, 288, 6, 6]         373,536\n",
            "     BatchNorm2d-163            [-1, 288, 6, 6]             576\n",
            "            ReLU-164            [-1, 288, 6, 6]               0\n",
            "       ConvBlock-165            [-1, 288, 6, 6]               0\n",
            "          Conv2d-166             [-1, 32, 6, 6]          16,416\n",
            "     BatchNorm2d-167             [-1, 32, 6, 6]              64\n",
            "            ReLU-168             [-1, 32, 6, 6]               0\n",
            "       ConvBlock-169             [-1, 32, 6, 6]               0\n",
            "          Conv2d-170             [-1, 64, 6, 6]          51,264\n",
            "     BatchNorm2d-171             [-1, 64, 6, 6]             128\n",
            "            ReLU-172             [-1, 64, 6, 6]               0\n",
            "       ConvBlock-173             [-1, 64, 6, 6]               0\n",
            "       MaxPool2d-174            [-1, 512, 6, 6]               0\n",
            "          Conv2d-175             [-1, 64, 6, 6]          32,832\n",
            "     BatchNorm2d-176             [-1, 64, 6, 6]             128\n",
            "            ReLU-177             [-1, 64, 6, 6]               0\n",
            "       ConvBlock-178             [-1, 64, 6, 6]               0\n",
            "       Inception-179            [-1, 528, 6, 6]               0\n",
            "AdaptiveAvgPool2d-180            [-1, 528, 4, 4]               0\n",
            "          Conv2d-181            [-1, 128, 4, 4]          67,712\n",
            "            ReLU-182            [-1, 128, 4, 4]               0\n",
            "          Linear-183                 [-1, 1024]       2,098,176\n",
            "            ReLU-184                 [-1, 1024]               0\n",
            "         Dropout-185                 [-1, 1024]               0\n",
            "          Linear-186                   [-1, 10]          10,250\n",
            "       Auxiliary-187                   [-1, 10]               0\n",
            "          Conv2d-188            [-1, 256, 6, 6]         135,424\n",
            "     BatchNorm2d-189            [-1, 256, 6, 6]             512\n",
            "            ReLU-190            [-1, 256, 6, 6]               0\n",
            "       ConvBlock-191            [-1, 256, 6, 6]               0\n",
            "          Conv2d-192            [-1, 160, 6, 6]          84,640\n",
            "     BatchNorm2d-193            [-1, 160, 6, 6]             320\n",
            "            ReLU-194            [-1, 160, 6, 6]               0\n",
            "       ConvBlock-195            [-1, 160, 6, 6]               0\n",
            "          Conv2d-196            [-1, 320, 6, 6]         461,120\n",
            "     BatchNorm2d-197            [-1, 320, 6, 6]             640\n",
            "            ReLU-198            [-1, 320, 6, 6]               0\n",
            "       ConvBlock-199            [-1, 320, 6, 6]               0\n",
            "          Conv2d-200             [-1, 32, 6, 6]          16,928\n",
            "     BatchNorm2d-201             [-1, 32, 6, 6]              64\n",
            "            ReLU-202             [-1, 32, 6, 6]               0\n",
            "       ConvBlock-203             [-1, 32, 6, 6]               0\n",
            "          Conv2d-204            [-1, 128, 6, 6]         102,528\n",
            "     BatchNorm2d-205            [-1, 128, 6, 6]             256\n",
            "            ReLU-206            [-1, 128, 6, 6]               0\n",
            "       ConvBlock-207            [-1, 128, 6, 6]               0\n",
            "       MaxPool2d-208            [-1, 528, 6, 6]               0\n",
            "          Conv2d-209            [-1, 128, 6, 6]          67,712\n",
            "     BatchNorm2d-210            [-1, 128, 6, 6]             256\n",
            "            ReLU-211            [-1, 128, 6, 6]               0\n",
            "       ConvBlock-212            [-1, 128, 6, 6]               0\n",
            "       Inception-213            [-1, 832, 6, 6]               0\n",
            "       MaxPool2d-214            [-1, 832, 3, 3]               0\n",
            "          Conv2d-215            [-1, 256, 3, 3]         213,248\n",
            "     BatchNorm2d-216            [-1, 256, 3, 3]             512\n",
            "            ReLU-217            [-1, 256, 3, 3]               0\n",
            "       ConvBlock-218            [-1, 256, 3, 3]               0\n",
            "          Conv2d-219            [-1, 160, 3, 3]         133,280\n",
            "     BatchNorm2d-220            [-1, 160, 3, 3]             320\n",
            "            ReLU-221            [-1, 160, 3, 3]               0\n",
            "       ConvBlock-222            [-1, 160, 3, 3]               0\n",
            "          Conv2d-223            [-1, 320, 3, 3]         461,120\n",
            "     BatchNorm2d-224            [-1, 320, 3, 3]             640\n",
            "            ReLU-225            [-1, 320, 3, 3]               0\n",
            "       ConvBlock-226            [-1, 320, 3, 3]               0\n",
            "          Conv2d-227             [-1, 32, 3, 3]          26,656\n",
            "     BatchNorm2d-228             [-1, 32, 3, 3]              64\n",
            "            ReLU-229             [-1, 32, 3, 3]               0\n",
            "       ConvBlock-230             [-1, 32, 3, 3]               0\n",
            "          Conv2d-231            [-1, 128, 3, 3]         102,528\n",
            "     BatchNorm2d-232            [-1, 128, 3, 3]             256\n",
            "            ReLU-233            [-1, 128, 3, 3]               0\n",
            "       ConvBlock-234            [-1, 128, 3, 3]               0\n",
            "       MaxPool2d-235            [-1, 832, 3, 3]               0\n",
            "          Conv2d-236            [-1, 128, 3, 3]         106,624\n",
            "     BatchNorm2d-237            [-1, 128, 3, 3]             256\n",
            "            ReLU-238            [-1, 128, 3, 3]               0\n",
            "       ConvBlock-239            [-1, 128, 3, 3]               0\n",
            "       Inception-240            [-1, 832, 3, 3]               0\n",
            "          Conv2d-241            [-1, 384, 3, 3]         319,872\n",
            "     BatchNorm2d-242            [-1, 384, 3, 3]             768\n",
            "            ReLU-243            [-1, 384, 3, 3]               0\n",
            "       ConvBlock-244            [-1, 384, 3, 3]               0\n",
            "          Conv2d-245            [-1, 192, 3, 3]         159,936\n",
            "     BatchNorm2d-246            [-1, 192, 3, 3]             384\n",
            "            ReLU-247            [-1, 192, 3, 3]               0\n",
            "       ConvBlock-248            [-1, 192, 3, 3]               0\n",
            "          Conv2d-249            [-1, 384, 3, 3]         663,936\n",
            "     BatchNorm2d-250            [-1, 384, 3, 3]             768\n",
            "            ReLU-251            [-1, 384, 3, 3]               0\n",
            "       ConvBlock-252            [-1, 384, 3, 3]               0\n",
            "          Conv2d-253             [-1, 48, 3, 3]          39,984\n",
            "     BatchNorm2d-254             [-1, 48, 3, 3]              96\n",
            "            ReLU-255             [-1, 48, 3, 3]               0\n",
            "       ConvBlock-256             [-1, 48, 3, 3]               0\n",
            "          Conv2d-257            [-1, 128, 3, 3]         153,728\n",
            "     BatchNorm2d-258            [-1, 128, 3, 3]             256\n",
            "            ReLU-259            [-1, 128, 3, 3]               0\n",
            "       ConvBlock-260            [-1, 128, 3, 3]               0\n",
            "       MaxPool2d-261            [-1, 832, 3, 3]               0\n",
            "          Conv2d-262            [-1, 128, 3, 3]         106,624\n",
            "     BatchNorm2d-263            [-1, 128, 3, 3]             256\n",
            "            ReLU-264            [-1, 128, 3, 3]               0\n",
            "       ConvBlock-265            [-1, 128, 3, 3]               0\n",
            "       Inception-266           [-1, 1024, 3, 3]               0\n",
            "AdaptiveAvgPool2d-267           [-1, 1024, 1, 1]               0\n",
            "         Dropout-268                 [-1, 1024]               0\n",
            "          Linear-269                   [-1, 10]          10,250\n",
            "================================================================\n",
            "Total params: 10,348,590\n",
            "Trainable params: 10,348,590\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.11\n",
            "Forward/backward pass size (MB): 22.05\n",
            "Params size (MB): 39.48\n",
            "Estimated Total Size (MB): 61.64\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "model = GoogLeNet()\n",
        "\n",
        "model.to(device)\n",
        "summary(model, (3, 96, 96))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvmCqZtRpHvG"
      },
      "source": [
        "## Loading CIFAR-10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "R8lAVfrwpHvG"
      },
      "outputs": [],
      "source": [
        "def cifar_dataloader():\n",
        "    \n",
        "    transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                    transforms.Normalize(mean=[0.5], std=[0.5])])\n",
        "            \n",
        "    # Input Data in Local Machine\n",
        "    # train_dataset = datasets.CIFAR10('../input_data', train=True, download=True, transform=transform)\n",
        "    # test_dataset = datasets.CIFAR10('../input_data', train=False, download=True, transform=transform)\n",
        "    \n",
        "    # Input Data in Google Drive\n",
        "    train_dataset = datasets.CIFAR10('/content/drive/MyDrive/All_Datasets/CIFAR10', train=True, download=True, transform=transform)\n",
        "    \n",
        "    test_dataset = datasets.CIFAR10('/content/drive/MyDrive/All_Datasets/CIFAR10', train=False, download=True, transform=transform)\n",
        "\n",
        "    # Split dataset into training set and validation set.\n",
        "    train_dataset, val_dataset = random_split(train_dataset, (45000, 5000))\n",
        "    \n",
        "    print(\"Image shape of a random sample image : {}\".format(train_dataset[0][0].numpy().shape), end = '\\n\\n')\n",
        "    \n",
        "    print(\"Training Set:   {} images\".format(len(train_dataset)))\n",
        "    print(\"Validation Set:   {} images\".format(len(val_dataset)))\n",
        "    print(\"Test Set:       {} images\".format(len(test_dataset)))\n",
        "    \n",
        "    BATCH_SIZE = 128\n",
        "\n",
        "    # Generate dataloader\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    \n",
        "    return train_loader, val_loader, test_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rsO079VsrW8h",
        "outputId": "9d6c9336-25ec-45c1-eabd-8a7a9a1fe4ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Image shape of a random sample image : (3, 32, 32)\n",
            "\n",
            "Training Set:   45000 images\n",
            "Validation Set:   5000 images\n",
            "Test Set:       10000 images\n"
          ]
        }
      ],
      "source": [
        "train_loader, val_loader, test_loader = cifar_dataloader()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keBxxxCurcxj"
      },
      "source": [
        "## c) Training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "8Iy7e81D6Cd9"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sk7Zk0eZrHpC",
        "outputId": "f76aef77-24fd-410e-d4a4-2ce8bd29fea8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[For Epoch 1/15]: train-loss = 2.40094 | train-acc = 0.453 | val-loss = 2.10815 | val-acc = 0.524\n",
            "[For Epoch 2/15]: train-loss = 1.72943 | train-acc = 0.624 | val-loss = 1.70226 | val-acc = 0.622\n",
            "[For Epoch 3/15]: train-loss = 1.40953 | train-acc = 0.699 | val-loss = 1.62197 | val-acc = 0.652\n",
            "[For Epoch 4/15]: train-loss = 1.18186 | train-acc = 0.750 | val-loss = 1.30169 | val-acc = 0.720\n",
            "[For Epoch 5/15]: train-loss = 1.00871 | train-acc = 0.789 | val-loss = 1.36148 | val-acc = 0.725\n",
            "[For Epoch 6/15]: train-loss = 0.87731 | train-acc = 0.817 | val-loss = 1.20267 | val-acc = 0.751\n",
            "[For Epoch 7/15]: train-loss = 0.75602 | train-acc = 0.840 | val-loss = 1.40170 | val-acc = 0.735\n",
            "[For Epoch 8/15]: train-loss = 0.66466 | train-acc = 0.859 | val-loss = 1.26839 | val-acc = 0.763\n",
            "[For Epoch 9/15]: train-loss = 0.56721 | train-acc = 0.881 | val-loss = 1.31561 | val-acc = 0.757\n",
            "[For Epoch 10/15]: train-loss = 0.49533 | train-acc = 0.898 | val-loss = 1.26927 | val-acc = 0.771\n",
            "[For Epoch 11/15]: train-loss = 0.41394 | train-acc = 0.913 | val-loss = 1.38300 | val-acc = 0.763\n",
            "[For Epoch 12/15]: train-loss = 0.36604 | train-acc = 0.922 | val-loss = 1.48668 | val-acc = 0.760\n",
            "[For Epoch 13/15]: train-loss = 0.31964 | train-acc = 0.933 | val-loss = 1.60355 | val-acc = 0.758\n",
            "[For Epoch 14/15]: train-loss = 0.28009 | train-acc = 0.943 | val-loss = 1.49877 | val-acc = 0.760\n",
            "[For Epoch 15/15]: train-loss = 0.25052 | train-acc = 0.947 | val-loss = 1.63167 | val-acc = 0.765\n"
          ]
        }
      ],
      "source": [
        "train_epoch_loss_history, val_epoch_loss_history = train_model(model, train_loader, val_loader, criterion, optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STo-5WyWQk9_"
      },
      "source": [
        "## Evaluating model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUQcUcsccRMg",
        "outputId": "d87c8eca-5977-43d8-ff9a-2b4f84cb8ac3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = GoogLeNet()\n",
        "model.load_state_dict(torch.load('/content/sample_data/googlenet_model'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wD4kZBywAScv",
        "outputId": "892868af-f8b4-4247-c7d1-56109f60e8c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy: 0.7571\n"
          ]
        }
      ],
      "source": [
        "num_test_samples = 10000\n",
        "correct = 0 \n",
        "\n",
        "model.eval().cuda()\n",
        "\n",
        "with  torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        # Make predictions.\n",
        "        prediction, _, _ = model(inputs)\n",
        "\n",
        "        # Retrieve predictions indexes.\n",
        "        _, predicted_class = torch.max(prediction.data, 1)\n",
        "\n",
        "        # Compute number of correct predictions.\n",
        "        correct += (predicted_class == labels).float().sum().item()\n",
        "\n",
        "test_accuracy = correct / num_test_samples\n",
        "\n",
        "print('Test accuracy: {}'.format(test_accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bonus Sections - GoogLeNet Basic Explanations as talked about the associated YouTube Video \n",
        "\n",
        "\n",
        "\n",
        "### Original Problem\n",
        "\n",
        "Salient parts in the image can have extremely large variation in size. For instance, an image with a cat can be either of the following, as shown below. The area occupied by the cat is different in each image.\n",
        "\n",
        "![Imgur](https://imgur.com/uAWiQaC.png)\n",
        "\n",
        "- Because of this huge variation in the location of the information, choosing the right kernel size for the convolution operation becomes tough. A larger kernel is preferred for information that is distributed more globally, and a smaller kernel is preferred for information that is distributed more locally.\n",
        "\n",
        "- Very deep networks are prone to overfitting. It also hard to pass gradient updates through the entire network.\n",
        "\n",
        "- Naively stacking large convolution operations is computationally expensive.\n",
        "\n",
        "The Solution:\n",
        "\n",
        "Why not have filters with multiple sizes operate on the same level? The network essentially would get a bit “wider” rather than “deeper”. The authors designed the inception module to reflect the same.\n",
        "\n",
        "The below image is the “naive” inception module. It performs convolution on an input, with 3 different sizes of filters (1x1, 3x3, 5x5). Additionally, max pooling is also performed. The outputs are concatenated and sent to the next inception module.\n",
        "\n",
        "![Imgur](https://imgur.com/x5pl2QB.png)\n",
        "\n",
        "However,  deep neural networks are computationally expensive. To make it cheaper, the authors limit the number of input channels by adding an extra 1x1 convolution before the 3x3 and 5x5 convolutions. Though adding an extra operation may seem counterintuitive, 1x1 convolutions are far more cheaper than 5x5 convolutions, and the reduced number of input channels also help. Do note that however, the 1x1 convolution is introduced after the max pooling layer, rather than before.\n",
        "\n",
        "\n",
        "![Imgur](https://imgur.com/zqrJ2jo.png)\n",
        "\n",
        "Using the dimension reduced inception module, a neural network architecture was built. This was popularly known as GoogLeNet (Inception v1). The architecture is shown below:\n",
        "\n",
        "\n",
        "Proposed Architectural Details\n",
        "The paper proposes a new type of architecture – GoogLeNet or Inception v1. It is basically a convolutional neural network (CNN) which is 27 layers deep. Below is the model summary:\n",
        "\n",
        "\n",
        "![Imgur](https://imgur.com/8Br0HLk.png)\n",
        "\n",
        "Notice in the above image that there is a layer called inception layer. This is actually the main idea behind the paper’s approach.\n",
        "\n",
        "![Imgur](https://imgur.com/B281dyr.png)\n",
        "\n",
        "\n",
        "### (Inception Layer) is a combination of all those layers (namely, 1×1 Convolutional layer, 3×3 Convolutional layer, 5×5 Convolutional layer) with their output filter banks concatenated into a single output vector forming the input of the next stage.\n",
        "\n",
        "\n",
        "===========================================================================\n",
        "\n",
        "### Inception Module:\n",
        "\n",
        "The inception module is different from previous architectures such as AlexNet, ZF-Net. In this architecture, there is a fixed convolution size for each layer.\n",
        "\n",
        "In theory you can have as many filter sizes as possible, but the Inception Architecture is restricted to filter sizes 1 × 1, 3 × 3 and 5 × 5. The small filters help capture the local details and features whereas spread out features of higher abstraction are captured by the larger filters. A 3 × 3 max pooling is also added to the Inception architecture, because, why not? Historically, it has been found that pooling layers make the network work better, so might as well add them!\n",
        "\n",
        "#### In the Inception module 1×1, 3×3, 5×5 convolution and 3×3 max pooling performed in a parallel way at the input and the output of these are stacked together to generate final output. The idea behind is - that the convolution filters of different sizes will handle objects at multiple scale better.\n",
        "\n",
        "\n",
        "==========================================================================="
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "MyWork_Final_GoogLeNet.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
